{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab Assignment 5\n",
        "## Learning Objectives\n",
        "* Demonstrate the capability of applying natural language processing in practice\n",
        "\n",
        "## Due Date\n",
        "**Midnight, Tuesday, December 12, 2023**\n",
        "\n",
        "## Assignment Submission Instructions\n",
        "When your file is ready, submit the following deliverables to the Lab Assignmen 5 dropbox:\n",
        "* Provide the link to your Google Colab notebook in the comments section; please make sure that **you enable the general access to your notebook with links before submission**. Failure to open your notebook will automatically lead to a grade of 0.\n",
        "* Upload the notebook file with the `.ipynb` suffix to the submission drop box. The uploaded notebook should have the same content as the one shared through the link, include enough documentation of the code, and have all the outputs available.\n",
        "\n",
        "## Others\n",
        "As always, feel free to come to our office hours or let us know through email if you face any difficulties/challenges while finishing the assignment. Good luck! For your convenience, I have created the text and code cells you might need for the lab assignment. Please also complete your contact information in the notebook as well."
      ],
      "metadata": {
        "id": "xakXlnwyz35c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Student's Contact Information:\n",
        "Name: Sarthak Haldar\n",
        "\n",
        "Email: sarthakhaldar@arizona.edu"
      ],
      "metadata": {
        "id": "2G_DjWZTpXld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 0: Download SMS Spam Dataset\n",
        "For this lab assignment, we will be working with the [SMS Spam Dataset](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) provided by the UCI machine learning repository.\n",
        "\n",
        "Fighting spams is a critical task for many mobile service providers. However, such a task is challenging because of the complexity of natural language. In this lab assignment, we will apply the natural language processing techniques discussed during the class meetings to build a classifier that can accurately predict whether an SMS text is a spam or not.\n",
        "\n",
        "For this lab assignment, you need to first download two text files, namely `spam-train.csv` and `spam-test.csv`. The `spam-train.txt` includes the text of multiple SMS messages as well as their labels (1 for spam and 0 for non-spam). The `spam-test.txt` includes text and labels for another set of SMS messages. Some example data points are listed as follows:\n",
        "```\n",
        "label\ttext\n",
        "0\t\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
        "1\t\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n",
        "0\t\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
        "1\t\tWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
        "1\t\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "AzDlcbWo0nUb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NmxUwJOOzvsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60eb881d-4dfa-4365-908c-faebd5e38c10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('spam-test.csv', <http.client.HTTPMessage at 0x7d3fd02b6a70>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from urllib.request import urlretrieve\n",
        "urlretrieve('https://drive.google.com/uc?export=download&id=1NdiA8ZXLrIkdOo1xH3EPlp7HB8KMamck',\n",
        "            'spam-train.csv')\n",
        "urlretrieve('https://drive.google.com/uc?export=download&id=1cMEw3JICGb_lGhYz186jbr-rhExQ6RCE',\n",
        "            'spam-test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Import Data (0.75 Point)\n",
        "In this section, you need to use **`pandas`** to import both `spam-train.csv` and `spam-test.csv`. Please create a DataFrame `spam_train` to store the data from `spam-train.csv` and create another DataFrame `spam_test` to store the data from `spam-test.csv`. Please also report how many SMS messages each csv file has."
      ],
      "metadata": {
        "id": "ilR3OZZfJd2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This code cell is for Lab Assignment 5 Part 1\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-bS2A5t-JdT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Loading spam-train.csv into spam_train DataFrame\n",
        "spam_train = pd.read_csv(\"spam-train.csv\")\n",
        "\n",
        "# Loading spam-test.csv into spam_test DataFrame\n",
        "spam_test = pd.read_csv(\"spam-test.csv\")\n",
        "\n",
        "# Displaying the number of SMS messages in each DataFrame\n",
        "num_train_messages = len(spam_train)\n",
        "num_test_messages = len(spam_test)\n",
        "\n",
        "print(f\"Number of SMS messages in spam-train.csv: {num_train_messages}\")\n",
        "print(f\"Number of SMS messages in spam-test.csv: {num_test_messages}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7q8lbhViLqS",
        "outputId": "ae76b45b-35f6-4808-a49c-567b7dfecdfa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of SMS messages in spam-train.csv: 1600\n",
            "Number of SMS messages in spam-test.csv: 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2.1: Preprocess Text Data (1 Point)\n",
        "In this section, you need to complete the code for preprocessing the SMS text data in both `spam-train.csv` and `spam-test.csv`. Specifically, the code should (1) tokenize each SMS text, (2) lowercase each token, (3) remove punctuations, (4) remove stop words, and (5) conduct stemming for each remaining token. Print the first 10 preprocessed SMS text messages in `spam-train.csv` and `spam-test.csv`, respectively.\n",
        "\n",
        "We have provided some example code below to help you get started. For this part, you only need to complete the code under the \"TODO\" comments, where the code needs to separately preprocess SMS messages for both `spam-train.csv` and `spam-test.csv`.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wBFvCLWkuhRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This code cell is for Lab Assignment 5 Part 2\n",
        "\"\"\"\n",
        "train_msgs = spam_train.iloc[:, 1]\n",
        "test_msgs = spam_test.iloc[:, 1]\n",
        "\n",
        "## download the punkt module\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "## get punctuations\n",
        "import string\n",
        "punctuations = string.punctuation\n",
        "\n",
        "## download stop words\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "## import stemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "## iteratively process each SMS message in spam-train.csv\n",
        "new_train_msgs = []\n",
        "for msg in train_msgs:\n",
        "    ## TODO: insert your code here to process each message in the spam-train.csv\n",
        "\n",
        "\n",
        "## iteratively process each SMS message in spam-test.csv\n",
        "new_test_msgs = []\n",
        "for msg in test_msgs:\n",
        "    ## TODO: insert your code here to process each message in the spam-test.csv"
      ],
      "metadata": {
        "id": "85vshgSzug3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_msgs = spam_train.iloc[:, 1]\n",
        "test_msgs = spam_test.iloc[:, 1]\n"
      ],
      "metadata": {
        "id": "3CSh4OVDvY3g"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## download the punkt module\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yc7JCPKbvaXu",
        "outputId": "af809802-1a05-4c5b-f699-fac2f4fc4142"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## get punctuations\n",
        "import string\n",
        "punctuations = string.punctuation"
      ],
      "metadata": {
        "id": "HdpEU-VSvd-r"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## download stop words\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5lBiaVYvgrv",
        "outputId": "7fc36ed8-ce0e-4a0f-b71b-776bcbe4cefc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## import stemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "UBhuXzGyvj77"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "id": "BWk6MVKVxse5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess train set"
      ],
      "metadata": {
        "id": "Hd205wZVylAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process each SMS message in spam-train.csv\n",
        "new_train_msgs = []\n",
        "for msg in train_msgs:\n",
        "    # Tokenize the words in the message\n",
        "    tokens = word_tokenize(msg)\n",
        "\n",
        "    # Lowercase each token\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "\n",
        "    # Remove punctuations\n",
        "    tokens = [word for word in tokens if word.lower() not in punctuations]\n",
        "\n",
        "    # Remove stop words\n",
        "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    # Apply stemming\n",
        "    tokens = [ps.stem(word) for word in tokens]\n",
        "\n",
        "    # Append the processed message to the new_train_msgs list\n",
        "    new_train_msgs.append(tokens)\n",
        "\n",
        "# Display the processed messages\n",
        "for i, msg in enumerate(new_train_msgs[:5]):  # Displaying the first 5 processed messages\n",
        "    print(f\"Processed Message {i + 1}: {msg}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdhBrLl4xNKc",
        "outputId": "0b3f6825-1e13-477a-f8f6-564eea97d42a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Message 1: ['yep', 'pretti', 'sculptur']\n",
            "Processed Message 2: ['leav']\n",
            "Processed Message 3: ['take', 'exam', 'march', '3']\n",
            "Processed Message 4: ['want', '750', 'anytim', 'network', 'min', '150', 'text', 'new', 'video', 'phone', 'five', 'pound', 'per', 'week', 'call', '08000776320', 'repli', 'deliveri', 'tomorrow']\n",
            "Processed Message 5: ['boy', 'made', 'fun', 'today', 'ok', 'problem', 'sent', 'one', 'messag', 'fun']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess test set"
      ],
      "metadata": {
        "id": "CvET99yyyusJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process each SMS message in spam-test.csv\n",
        "new_test_msgs = []\n",
        "for msg in test_msgs:\n",
        "    # Tokenize the words in the message\n",
        "    tokens = word_tokenize(msg)\n",
        "\n",
        "    # Lowercase each token\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "\n",
        "    # Remove punctuations\n",
        "    tokens = [word for word in tokens if word.lower() not in punctuations]\n",
        "\n",
        "    # Remove stop words\n",
        "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    # Apply stemming\n",
        "    tokens = [ps.stem(word) for word in tokens]\n",
        "\n",
        "    # Append the processed message to the new_train_msgs list\n",
        "    new_test_msgs.append(tokens)\n",
        "\n",
        "# Display the processed messages\n",
        "for i, msg in enumerate(new_test_msgs[:5]):  # Displaying the first 5 processed messages\n",
        "    print(f\"Processed Message {i + 1}: {msg}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42c0897f-dad7-4daa-a259-8bcefec8f301",
        "id": "5OFTr98ADstG"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Message 1: ['sorri', 'da', 'thought', 'call', 'lot', 'time', 'lil', 'busy.i', 'call', 'noon', '..']\n",
            "Processed Message 2: ['urgent', 'mobil', 'award', '£2,000', 'bonu', 'caller', 'prize', '1/08/03', '2nd', 'attempt', 'contact', 'call', '0871-4719-523', 'box95qu', 'bt', 'nation', 'rate']\n",
            "Processed Message 3: ['u', 'call', '...']\n",
            "Processed Message 4: ['wait', '4', 'u', 'lor', '...', 'need', '2', 'feel', 'bad', 'lar', '...']\n",
            "Processed Message 5: ['win', 'winner', 'mr.', 't.', 'foley', 'ipod', 'excit', 'prize', 'soon', 'keep', 'eye', 'ur', 'mobil', 'visit', 'www.win-82050.co.uk']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2.2: Convert Text Data with TF-IDF (0.75 Point)\n",
        "After data preprocessing, the next step is to convert each SMS message into a numerical vector using TF-IDF. We also provide some example code to help you get started, so you only need to complete the code under the \"TODO\" comments.\n",
        "\n",
        "For this section, your code should convert each SMS text into a TF-IDF vector so that the input features will become a large matrix of `(n_msg, n_features)`, where `n_msg` is the total number of SMS messages in the dataset, and `n_features` is the total number of features (i.e., the number of top tokens used for building the TF-IDF vectors).\n",
        "\n",
        "When completing the code, please notice that we should fit the TF-IDF vectorizer using data from `spam-train.csv` and then use the vectorizer to convert data from both `spam-train.csv` and `spam-test.csv`. This is because some tokens might only appear in the test set and including them to build the TF-IDF vectors would cause potential [data leakage](https://machinelearningmastery.com/data-leakage-machine-learning/)."
      ],
      "metadata": {
        "id": "AZJE_hjfAMi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we can apply TfidfVectorizer function to extract TF-IDF vectors for each message\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_transformer = TfidfVectorizer(max_features=500)\n",
        "\n",
        "# TODO: replace the question marks '?' with the correct variables you created in Section 2.1\n",
        "tfidf_transformer.fit('?')\n",
        "train_features = tfidf_transformer.transform('?').toarray()\n",
        "test_features = tfidf_transformer.transform('?').toarray()"
      ],
      "metadata": {
        "id": "N_jlHJnBbok7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining the processed messages back into text\n",
        "train_texts = [' '.join(tokens) for tokens in new_train_msgs]\n",
        "test_texts = [' '.join(tokens) for tokens in new_test_msgs]\n"
      ],
      "metadata": {
        "id": "Vs-zopitIgvX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the TfidfVectorizer with max_features=500\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_transformer = TfidfVectorizer(max_features=500)"
      ],
      "metadata": {
        "id": "882SytwjIn9Q"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the TfidfVectorizer using data from spam-train.csv\n",
        "tfidf_transformer.fit(train_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "_p8nwD0OJcRZ",
        "outputId": "e9fef4e2-79de-40ff-a043-c577a2c4408d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(max_features=500)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(max_features=500)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=500)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform both train and test data into TF-IDF vectors\n",
        "train_features = tfidf_transformer.transform(train_texts).toarray()\n",
        "test_features = tfidf_transformer.transform(test_texts).toarray()"
      ],
      "metadata": {
        "id": "jc5om2C6Jj3b"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Apply Machine Learning to Combat Spam Messages (1.5 Points)\n",
        "In this section, you need to train a Naive Bayes classifier to predict whether a given SMS message is a spam message or not. Specifically, you need to use the TF-IDF vectors of the training dataset (`train_features`) to train the classifier and use the TF-IDF vectors of the test dataset (`test_features`) for prediction. Once the prediction is finished, please report the F1 score and AUC-ROC on the test dataset for the Naive Bayes classifier.\n",
        "\n",
        "For your reference, we have provided example code to train a logistic regression classifier using the TF-IDF vectors of the training dataset and calculate the classifier's F1  and AUC-ROC scores on the test dataset. In addition, we also show how you can extract each token's coefficient from the trained logistic regression model (this part will be relevant to the extra lab assignment).\n",
        "\n"
      ],
      "metadata": {
        "id": "c7Q2zLvju5mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In this code cell, we provide example code on how to evaluate a logistic regression\n",
        "classifier's performance in identifying spam messages;\n",
        "we also provide code on extracting the logistic regression coefficients for\n",
        "different tokens, which could be helpful for your extra lab assignment\n",
        "\"\"\"\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "\n",
        "log_clf = LogisticRegression()\n",
        "log_clf.fit(train_features, spam_train['label'])        # fit the model\n",
        "log_clf_pred = log_clf.predict(test_features)           # make predictions\n",
        "log_clf_score = log_clf.predict_proba(test_features)    # get prediction scores\n",
        "\n",
        "## F1 score\n",
        "log_clf_f1 = f1_score(spam_test['label'], log_clf_pred)\n",
        "print('Prediction F1: {:.4f}'.format(log_clf_f1))\n",
        "\n",
        "## AUC-ROC\n",
        "log_clf_auc = roc_auc_score(spam_test['label'], log_clf_score[:, 1])\n",
        "print('AUC-ROC : {:.4f}'.format(log_clf_auc))\n",
        "print()\n",
        "\n",
        "# you can use the code below to extract logistic regression's features and their\n",
        "# coefficients; specifically, feature_names_in_ will return all the input features,\n",
        "# and coef_ will return all the features' estimated coefficients\n",
        "log_clf_coef = pd.DataFrame({\n",
        "    'Feature Name': tfidf_transformer.get_feature_names_out(),\n",
        "    'Coefficient': log_clf.coef_[0]\n",
        "})\n",
        "print(log_clf_coef)"
      ],
      "metadata": {
        "id": "_WE5UPXSu46C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In this code cell, please write your own code on evaluating a naive bayes\n",
        "classifier's performance in identifying spam messages\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-hlKWy7biQRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "metadata": {
        "id": "I7isNrDqNyEt"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the Naive Bayes classifier\n",
        "nb_clf = MultinomialNB()"
      ],
      "metadata": {
        "id": "_PCgT927N1-K"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the Naive Bayes classifier using TF-IDF vectors of the training dataset\n",
        "nb_clf.fit(train_features, spam_train['label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "7BI1QQ3POPmo",
        "outputId": "d756deb6-0ec5-4729-f438-9afd9b662804"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions on the test dataset\n",
        "nb_clf_pred = nb_clf.predict(test_features)"
      ],
      "metadata": {
        "id": "6h-CMbuBOU7j"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting prediction scores (probabilities)\n",
        "nb_clf_score = nb_clf.predict_proba(test_features)\n"
      ],
      "metadata": {
        "id": "XKEWsAnsObbM"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# F1 score\n",
        "nb_clf_f1 = f1_score(spam_test['label'], nb_clf_pred)\n",
        "print('Naive Bayes Prediction F1: {:.4f}'.format(nb_clf_f1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9v6ZFBwOh_E",
        "outputId": "edfa1e63-e36d-428f-da05-c4837a646db5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Prediction F1: 0.8468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AUC-ROC\n",
        "nb_clf_auc = roc_auc_score(spam_test['label'], nb_clf_score[:, 1])\n",
        "print('Naive Bayes AUC-ROC: {:.4f}'.format(nb_clf_auc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i_MAi_kOnCu",
        "outputId": "5b4ed7fc-2390-4b2d-ba52-8f2db0eb96b2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes AUC-ROC: 0.9905\n"
          ]
        }
      ]
    }
  ]
}